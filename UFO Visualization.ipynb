{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import (\n",
    "    ColumnDataSource,\n",
    "    HoverTool,\n",
    "    LogColorMapper,\n",
    "    Range1d, CustomJS, Slider\n",
    ")\n",
    "from bokeh.palettes import RdBu11 as palette\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import row, widgetbox\n",
    "from bokeh.sampledata.us_counties import data as counties\n",
    "from bokeh.sampledata.us_states import data as states\n",
    "from bokeh.sampledata.unemployment import data as unemployment\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from ufo_parser import DurationParser \n",
    "from ufo_downloader import LocationFinder\n",
    "\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_accumulated ={}\n",
    "available_state_codes = states.keys()\n",
    "\n",
    "for key, value in counties.items():\n",
    "    state_name = value[\"state\"].upper()\n",
    "    if state_name in states.keys() and \"number\" not in states[state_name]:\n",
    "        states[state_name][\"number\"] = key[0]\n",
    "\n",
    "for key,state in states.items():\n",
    "    state[\"code\"] = key\n",
    "\n",
    "state_list = []\n",
    "\n",
    "for key,state in states.items():\n",
    "    state_list.append(state)\n",
    "\n",
    "state_df = pd.DataFrame(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_data = []\n",
    "with open(\"data.json\",\"r\") as f:\n",
    "    ufo_data = json.load(f)\n",
    "    \n",
    "#raw_data = pandas.read_csv(\"data.csv\", sep=\";\", encoding=\"UTF8\", header=None, skiprows=1)\n",
    "ufo_df = pd.DataFrame.from_records(ufo_data)\n",
    "ufo_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.groupby([data[\"Date\"].dt.year,data[\"State\"]]).count()[\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "lf = LocationFinder(\"location_cache.json\")\n",
    "durparse = DurationParser()\n",
    "STATES = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \\\n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \\\n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \\\n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \\\n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "data, wrong_data, skipped_lines = [], [], 0\n",
    "for _, d in ufo_df.iterrows():\n",
    "    try:\n",
    "        # Transform or validate data\n",
    "        date = pd.to_datetime(d[\"Date\"])\n",
    "    except Exception as e:\n",
    "        date = pd.NaT\n",
    "    \n",
    "    try:\n",
    "        posted = pd.to_datetime(d[\"Posted\"])\n",
    "    except Exception as e:\n",
    "        posted = pd.NaT\n",
    "    \n",
    "    # State transformation and validation against prebuilt set.\n",
    "    state = d[\"State\"]\n",
    "    if state is not pd.np.NaN:\n",
    "        state = d[\"State\"].upper()\n",
    "        if state not in STATES:\n",
    "            state = pd.np.NaN\n",
    "    \n",
    "    # Unknown shapes are, according to the website, categorized as unspecified\n",
    "    shape = d[\"Shape\"].lower() if d[\"Shape\"] is not pd.np.NaN else \"unspecified\"\n",
    "    # Use the city parser to get geolocation\n",
    "    #res,http_used = lf.find_ugly(city=d[\"City\"],state_code=d[\"State\"],http_used=True)\n",
    "    #lat = res[\"latitude\"] if res[\"latitude\"] is not None else pd.np.NaN\n",
    "    #lon = res[\"longitude\"] if res[\"longitude\"] is not None else pd.np.NaN\n",
    "    lat,lon,conf = pd.np.NAN,pd.np.NAN,pd.np.NAN\n",
    "    \n",
    "    #if http_used:\n",
    "    #    print(d[\"City\"])\n",
    "    \n",
    "    # Use the duration parser to parse duration\n",
    "    duration = DurationParser.parse_duration(d[\"Duration\"])\n",
    "    row = (date, d[\"City\"], state, shape, duration, d[\"Summary\"], posted, d[\"link\"], lat, lon, conf)\n",
    "\n",
    "    # Do some checks...\n",
    "    if date.date() > posted.date():\n",
    "        wrong_data.append(row)\n",
    "        continue\n",
    "\n",
    "    # Add data to cleaned array\n",
    "    data.append(row)\n",
    "\n",
    "names = [\"Date\", \"City\", \"State\", \"Shape\", \"Duration\", \"Summary\", \"Posted\", \"Link\",\"Lat\",\"Lon\",\"Confidence\"]\n",
    "data = pd.DataFrame.from_records(data, columns=names)\n",
    "wrong_data = pd.DataFrame.from_records(wrong_data, columns=names)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pyspark import SQLContext\n",
    "from operator import add\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = pd.read_pickle(\"data_dump.pckl\")\n",
    "sql_context = SQLContext(sc)\n",
    "#rdd = sql_context.createDataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.read.format(\"csv\").options(header=True).load(\"data_dump.csv\")\n",
    "rdd.registerTempTable(\"data\")\n",
    "res = sql_context.sql(\"SELECT YEAR(Date) as Year,State,count(*) as values FROM data group by YEAR(Date),State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1967,\n",
       " 1968,\n",
       " 1969,\n",
       " 1970,\n",
       " 1971,\n",
       " 1972,\n",
       " 1973,\n",
       " 1974,\n",
       " 1975,\n",
       " 1976,\n",
       " 1977,\n",
       " 1978,\n",
       " 1979,\n",
       " 1980,\n",
       " 1981,\n",
       " 1982,\n",
       " 1983,\n",
       " 1984,\n",
       " 1985,\n",
       " 1986,\n",
       " 1987,\n",
       " 1988,\n",
       " 1989,\n",
       " 1990,\n",
       " 1991,\n",
       " 1992,\n",
       " 1993,\n",
       " 1994,\n",
       " 1995,\n",
       " 1996,\n",
       " 1997,\n",
       " 1998,\n",
       " 1999,\n",
       " 2000,\n",
       " 2001,\n",
       " 2002,\n",
       " 2003,\n",
       " 2004,\n",
       " 2005,\n",
       " 2006,\n",
       " 2007,\n",
       " 2008,\n",
       " 2009,\n",
       " 2010,\n",
       " 2011,\n",
       " 2012,\n",
       " 2013,\n",
       " 2014,\n",
       " 2015,\n",
       " 2016,\n",
       " 2017]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#r = res.groupby(res.Year).sum().collect()[0]\n",
    "#r.asDict()[\"sum(values)\"]\n",
    "\n",
    "years = [i.Year for i in res.groupby(res.Year).sum().collect() if i.Year is not None and i.asDict()[\"sum(values)\"] > 100 ]\n",
    "sorted(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_dump.csv\",\"w\",encoding=\"utf8\") as f:\n",
    "    data.to_csv(f)\n",
    "with open(\"data_dump.pckl\",\"wb\") as f:\n",
    "    data.to_pickle(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color_mapper = LogColorMapper(palette=palette)\n",
    "\n",
    "state_xy = (list(state_df[\"lons\"].values),list(state_df[\"lats\"].values))\n",
    "\n",
    "max_x = max([max(l) for l in state_xy[0]])\n",
    "max_y = max([max(l) for l in state_xy[1]])\n",
    "min_x = min([min(l) for l in state_xy[0]])\n",
    "min_y = min([min(l) for l in state_xy[1]])\n",
    "\n",
    "data=dict(\n",
    "    x=state_xy[0],\n",
    "    y=state_xy[1],\n",
    "    name=list(state_df[\"name\"].values),\n",
    "    used = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    ")\n",
    "\n",
    "data['1999'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2000'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2001'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2002'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2003'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2004'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2005'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2006'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2007'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "data['2008'] = [random.randrange(0,50) for i in range(len(state_xy[0]))]\n",
    "\n",
    "source = ColumnDataSource(data)\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,reset,hover,save\"\n",
    "\n",
    "p = figure(\n",
    "    title=\"States\", tools=TOOLS,\n",
    "    x_axis_location=None, y_axis_location=None\n",
    ")\n",
    "p.width=450\n",
    "p.height = 450\n",
    "p.x_range= Range1d(-170,-60)\n",
    "p.y_range = Range1d(min_y-10,max_y+10)\n",
    "p.grid.grid_line_color = None\n",
    "\n",
    "renderer = p.patches('x', 'y', source=source,\n",
    "          fill_color={'field': 'used', 'transform': color_mapper},\n",
    "          fill_alpha=0.7, line_color=\"white\", line_width=0.5)\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "hover.tooltips = [\n",
    "    (\"Name\", \"@name\"),\n",
    "    (\"Unemployment rate)\", \"@used%\"),\n",
    "    (\"(Long, Lat)\", \"($x, $y)\"),\n",
    "]\n",
    "\n",
    "callback = CustomJS(args=dict(source=source,plot=p,color_mapper = color_mapper,renderer = renderer), code=\"\"\"\n",
    "    var data = source.data;\n",
    "    var year = year.value;\n",
    "    used = data['used']\n",
    "    should_be = data[String(year)]\n",
    "    for (i = 0; i < should_be.length; i++) {\n",
    "        used[i] = should_be[i];\n",
    "    } \n",
    "    source.change.emit();\n",
    "    //source.trigger('change');\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "year_slider = Slider(start=1999, end=2008, value=1999, step=1,\n",
    "                    title=\"year\", callback=callback)\n",
    "callback.args[\"year\"] = year_slider\n",
    "\n",
    "layout = row(\n",
    "    p,\n",
    "    widgetbox(year_slider),\n",
    ")\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lons = []\n",
    "for item in state_df[\"lons\"]:\n",
    "    lons.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_val,min_val = -1000,2000\n",
    "for item in lons:\n",
    "    for i in item:\n",
    "        if i < min_val:\n",
    "            min_val = i\n",
    "        if i > max_val:\n",
    "            max_val = i\n",
    "print(min_val,max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
